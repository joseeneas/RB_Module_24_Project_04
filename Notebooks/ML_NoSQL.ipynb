{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a57c4c6",
   "metadata": {},
   "source": [
    "ML model for Formula One."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecdd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import datetime \n",
    "import tensorflow                                      as tf\n",
    "import shutil                                          as shu\n",
    "import warnings                                        as warn\n",
    "import pandas                                          as pd\n",
    "import matplotlib.pyplot                               as plt\n",
    "from   pymongo                 import MongoClient\n",
    "from   pprint                  import pprint\n",
    "from   colorama                import Style            as st\n",
    "from   colorama                import Fore             \n",
    "from   colorama                import Back             as bk\n",
    "from   sklearn.model_selection import train_test_split as tts\n",
    "from   sklearn.preprocessing   import StandardScaler   as sts\n",
    "from   tensorflow              import keras            as ker\n",
    "from   keras.models            import Sequential       as seq\n",
    "from   keras.layers            import Dense            as den\n",
    "from   sklearn.metrics         import classification_report\n",
    "\n",
    "warn.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Environment Setup\n",
    "#\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "w, h       = shu.get_terminal_size()\n",
    "\n",
    "def printSeparator():\n",
    "    print(Fore.GREEN + '-' * w + Fore.WHITE)\n",
    "    \n",
    "def logStep(msg):\n",
    "    l1 = len(msg)\n",
    "    l2 = w - l1\n",
    "    print(Fore.WHITE + str(datetime.datetime.now()) +  \" \" + Fore.YELLOW + msg + Fore.WHITE + \"-\" * l2  )\n",
    "    sys.stdout.flush()\n",
    "\n",
    "logStep(\"ENVIRONMENT PREPARATION\")\n",
    "warn.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "def printDFinfo(name,dfName):\n",
    "    printSeparator()\n",
    "    print('Name: ',name)\n",
    "    printSeparator()\n",
    "    print(dfName.info())\n",
    "    printSeparator()\n",
    "    print(f'Row Count :{Fore.RED}')\n",
    "    print(dfName.count(),Fore.WHITE)\n",
    "    printSeparator()\n",
    "    print(dfName.head())\n",
    "    printSeparator()\n",
    "\n",
    "def runtime_Diff(step_Number, step_Message1,base_SQL, cached_SQL):\n",
    "  start_time          = datetime.datetime.now()\n",
    "  logStep(step_Number + \" - RUNTIME DIFFERENCE\")\n",
    "  time_difference     = base_SQL - cached_SQL\n",
    "  logStep(F\"{step_Number} - Time required for a non-cached Query : {base_SQL}\")\n",
    "  logStep(F\"{step_Number} - Time required for a cached/part Query: {cached_SQL}\")\n",
    "  logStep(F\"{step_Number} - Time difference                      : {time_difference}\")\n",
    "  logStep(step_Number + \" - DONE\")\n",
    "  end_time            = datetime.datetime.now()\n",
    "  step_elapsed_time   = end_time - start_time\n",
    "  logStep(F\"{step_Number} - ELAPSED TIME: {step_elapsed_time} seconds\")\n",
    "  return step_elapsed_time\n",
    "\n",
    "print(F\"Copyright                              : {sys.copyright}\")\n",
    "print(F\"OS Platform                            : {sys.platform}\")\n",
    "print(F\"OS Name                                : {os.name}\")\n",
    "print(F\"OS HOME                                : {os.environ.get('HOME')}\")\n",
    "print(F\"OS uName                               : {os.uname().sysname}\")\n",
    "print(F\"OS NodeName                            : {os.uname().nodename}\")\n",
    "print(F\"OS Release                             : {os.uname().release}\")\n",
    "print(F\"OS Release Ver                         : {os.uname().version}\")\n",
    "print(F\"OS Machine                             : {os.uname().machine}\")\n",
    "print(F\"Process ID                             : {os.getpid()}\")\n",
    "print(F\"Parent Process                         : {os.getppid()}\")\n",
    "print(F\"OS User                                : {os.getlogin()}\")\n",
    "print(F\"OS User ID                             : {os.getuid()}\")\n",
    "print(F\"OS Group ID                            : {os.getgid()}\")\n",
    "print(F\"OS Effective ID                        : {os.geteuid()}\")\n",
    "print(F\"OS Effective GID                       : {os.getegid()}\")\n",
    "print(F\"Current dir                            : {os.getcwd()}\")\n",
    "print(F\"Python version                         : {sys.version}\")\n",
    "print(F\"Version info                           : {sys.version_info}\")\n",
    "print(F\"Python API Ver                         : {sys.api_version}\")\n",
    "print(F\"Executable                             : {sys.executable}\")\n",
    "print(F\"Spark UI                               : http://localhost:4040\")\n",
    "print(F\"Spark submit                           : {sys.argv[0]}\")\n",
    "print(F\"Hadoop Home                            : {os.environ.get('HADOOP_HOME')}\")\n",
    "print(F\"Java Home                              : {os.environ.get('JAVA_HOME')}\")\n",
    "print(F\"Current Working Directory              : {os.getcwd()}\")\n",
    "\n",
    "logStep(\"DONE\");\n",
    "end_time            = datetime.datetime.now()\n",
    "step00_elapsed_time = end_time - start_time\n",
    "logStep(F\"ELAPSED TIME: {step00_elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59390c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logStep('Environment Preparation')\n",
    "\n",
    "mongo = MongoClient(port=27017)\n",
    "print(mongo.list_database_names())\n",
    "db    = mongo['F1']\n",
    "print(db.list_collection_names())\n",
    "Results = db['Results']\n",
    "logStep('Environment Preparation Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the new restaurant was inserted\n",
    "# Filter results by name\n",
    "logStep('Model Preparation')\n",
    "\n",
    "query  = ({\"Position\" : {\"$gt\": 0}})\n",
    "fields = {'Position'  : 1,\n",
    "          'Driver': 1,\n",
    "          'Starting Grid': 1,\n",
    "          'Laps': 1,\n",
    "          'Time/Retired': 1,\n",
    "          'Points': 1}\n",
    "results = Results.find(query,fields)\n",
    "results_df = pd.DataFrame(list(results))\n",
    "results_df = results_df.drop(columns=['_id'])\n",
    "print(results_df.head())\n",
    "logStep('Model Preparation Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "printDFinfo('results_df',results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2783c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Log the processing progress\n",
    "#\n",
    "\n",
    "logStep('Model Preparation')\n",
    "    \n",
    "# \n",
    "# Determine the number of unique values in each column.\n",
    "#\n",
    "\n",
    "printSeparator()\n",
    "print('results_df.nunique()')\n",
    "printSeparator()\n",
    "print(results_df.nunique())\n",
    "printSeparator()\n",
    "\n",
    "logStep('Model Preparation')\n",
    "    \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Log the processing progress\n",
    "#\n",
    "\n",
    "logStep('For columns that have more than 10 unique values, determine the number of data points for each unique value.')\n",
    "    \n",
    "\n",
    "for column_Name in results_df.columns:\n",
    "    if results_df[column_Name].nunique() > 10:\n",
    "        print('Column Name', column_Name)\n",
    "        printSeparator()\n",
    "        print(results_df[column_Name].value_counts())\n",
    "        printSeparator()\n",
    "\n",
    "\n",
    "logStep('determined the number of data points for each unique value.')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5aa1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.insert(0,'Champion',0)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e70bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Log the processing progress\n",
    "#\n",
    "\n",
    "logStep('Use  pd.get_dummies()  to encode categorical variables.')\n",
    "#\n",
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "#\n",
    "\n",
    "results_df_numeric = pd.get_dummies(results_df)\n",
    "printDFinfo('results_df_numeric',results_df_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412bc8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Log the processing progress\n",
    "#\n",
    "\n",
    "logStep('Preprocess the Data')\n",
    "\n",
    "# \n",
    "# Split our preprocessed data into our features and target arrays\n",
    "#\n",
    "results_df_numeric.dropna()\n",
    "X = results_df_numeric.drop(['Champion'], axis=1)\n",
    "y = results_df_numeric['Champion']\n",
    "\n",
    "#\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "#\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(X, y, random_state=58)\n",
    "\n",
    "print('X_train.shape')\n",
    "print(X_train.shape)\n",
    "printSeparator()\n",
    "print('X_test.shape')\n",
    "print(X_test.shape)\n",
    "printSeparator()\n",
    "print('y_train.shape')\n",
    "print(y_train.shape)\n",
    "printSeparator()\n",
    "print('y_test.shape')\n",
    "print(y_test.shape)\n",
    "printSeparator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af6dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Log the processing progress\n",
    "#\n",
    "\n",
    "logStep('Scale the training and testing features datasets')\n",
    "\n",
    "#\n",
    "# Create a Standard Scaler instance\n",
    "# Fit the Standard Scaler\n",
    "# Scale the data\n",
    "#\n",
    "\n",
    "scaler         = sts()\n",
    "X_scaler       = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled  = X_scaler.transform(X_test)\n",
    "\n",
    "#\n",
    "# Log the processing progress\n",
    "#\n",
    "\n",
    "print('X_train_scaled.shape')\n",
    "print(X_train_scaled.shape)\n",
    "printSeparator()\n",
    "print('X_test_scaled.shape')\n",
    "print(X_test_scaled.shape)\n",
    "printSeparator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc74c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Log the processing progress\n",
    "#\n",
    "\n",
    "logStep('Define the model parameters')\n",
    "\n",
    "# \n",
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "#\n",
    "\n",
    "number_input_features = len(X_train_scaled[0]) # type: ignore\n",
    "hidden_nodes_layer1   = 12\n",
    "hidden_nodes_layer2   = 12\n",
    "\n",
    "nn_model = seq()\n",
    "\n",
    "#\n",
    "# First hidden layer\n",
    "#\n",
    "\n",
    "nn_model.add(den(units=hidden_nodes_layer1,input_dim=number_input_features, activation=\"tanh\"))\n",
    "\n",
    "#\n",
    "# Second hidden layer\n",
    "#\n",
    "\n",
    "nn_model.add(den(units=hidden_nodes_layer2, activation=\"tanh\"))\n",
    "\n",
    "#\n",
    "# Output layer\n",
    "#\n",
    "\n",
    "nn_model.add(den(units=1, activation=\"tanh\"))\n",
    "\n",
    "#\n",
    "# Check the structure of the model\n",
    "#\n",
    "\n",
    "nn_model.summary()\n",
    "printSeparator()\n",
    "#\n",
    "# Log the processing progress\n",
    "#\n",
    "\n",
    "logStep('10 - Compile the model')\n",
    "\n",
    "# \n",
    "# Compile the model\n",
    "#\n",
    "\n",
    "nn_model.compile(loss      = 'binary_crossentropy', \n",
    "                 optimizer = 'adam', \n",
    "                 metrics   = ['accuracy'])\n",
    "print('Model compiled')\n",
    "printSeparator()\n",
    "\n",
    "#\n",
    "# Log the processing progress\n",
    "#\n",
    "\n",
    "logStep('Train the model parameters')\n",
    "# \n",
    "# Train the model\n",
    "#\n",
    "\n",
    "print('Model Training')\n",
    "printSeparator()\n",
    "\n",
    "fit_model = nn_model.fit(X_train_scaled,y_train,epochs=50)\n",
    "\n",
    "printSeparator()\n",
    "print('Model Training Complete')\n",
    "printSeparator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Log the processing progress\n",
    "#\n",
    "\n",
    "logStep('Evaluate the model parameters')\n",
    "# \n",
    "# Evaluate the model using the test data\n",
    "#\n",
    "\n",
    "skip_optimization = False\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test)\n",
    "print(f\"Loss: {model_loss:2.2f}, Accuracy: {model_accuracy:2.2f}\")\n",
    "accuracy = model_accuracy * 100\n",
    "if (accuracy < 75):   \n",
    "    print(F'Accuracy is {accuracy:2.2f}%, less than 75%')\n",
    "    print(\"More optimization is required\")\n",
    "else:\n",
    "    print(F'Accuracy is {accuracy:2.2f}%, greater than or equal to 75%')\n",
    "    print(\"Model is optimized\")\n",
    "    history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))\n",
    "    history_df.plot(y=\"accuracy\")\n",
    "    plt.show()\n",
    "    history_df.plot(y=\"loss\",color='red')\n",
    "    plt.show()\n",
    "    #\n",
    "    # Log the processing progress\n",
    "    #\n",
    "    logStep('13 - Export the model to a HDF5 file')\n",
    "    # \n",
    "    # Export our model to HDF5 file\n",
    "    #\n",
    "    filename = 'Output/F1_model.h5'\n",
    "    #\n",
    "    # Save the model to a HDF5 file\n",
    "    #\n",
    "\n",
    "    nn_model.save(filename)\n",
    "    printSeparator()\n",
    "    print('Model saved to file : ',filename)\n",
    "    printSeparator()\n",
    "    print('End of processing')\n",
    "    skip_optimization = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a42f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Log the processing progress\n",
    "#\n",
    "if skip_optimization == False:\n",
    "  logStep('Attempt 2 - Add more neurons to a hidden layer')\n",
    "  number_input_features = len(X_train_scaled[0]) # type: ignore\n",
    "  print('Number of input features : ',number_input_features)\n",
    "  for nodes in range(14, 140, 2):\n",
    "    printSeparator()\n",
    "    print('Number of hidden nodes   : ',nodes)\n",
    "    hidden_nodes_layer1   = nodes\n",
    "    hidden_nodes_layer2   = nodes\n",
    "    hidden_nodes_layer3   = nodes\n",
    "    hidden_nodes_layer4   = nodes\n",
    "    hidden_nodes_layer5   = nodes\n",
    "    nn_model2             = seq(name=f\"Optimized_Model_{nodes}\")\n",
    "    nn_model2.add(den(units=hidden_nodes_layer1,input_dim=number_input_features, activation=\"tanh\"))\n",
    "    nn_model2.add(den(units=hidden_nodes_layer2, activation=\"tanh\"))\n",
    "    nn_model2.add(den(units=hidden_nodes_layer3, activation=\"tanh\"))\n",
    "    nn_model2.add(den(units=hidden_nodes_layer4, activation=\"tanh\"))\n",
    "    nn_model2.add(den(units=hidden_nodes_layer5, activation=\"tanh\"))\n",
    "    nn_model2.add(den(units=1,activation=\"tanh\"))\n",
    "    nn_model2.summary()\n",
    "    printSeparator()\n",
    "    print('Compile the model')\n",
    "    nn_model2.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics   = ['accuracy'])\n",
    "    print('Model compiled')\n",
    "    printSeparator()\n",
    "    print('Fit the model')\n",
    "    fit_model2 = nn_model2.fit(X_train_scaled,y_train,epochs=50)\n",
    "    print('Model fit')\n",
    "    printSeparator()\n",
    "    print('Evaluate the model')\n",
    "    model_loss2, model_accuracy2 = nn_model2.evaluate(X_test_scaled,y_test)\n",
    "    print(f\"Loss: {model_loss2:2.2f}, Accuracy: {model_accuracy2:2.2f}\")\n",
    "    accuracy2 = model_accuracy2 * 100\n",
    "    if (accuracy2 < 75):   \n",
    "      print(F'Accuracy is {accuracy2:2.2f}, less than 75%')\n",
    "      print(\"More optimization is required\")\n",
    "    else:\n",
    "      print(F'Accuracy is {accuracy2:2.2f}, greater than or equal to 75%')\n",
    "      print(\"Model is optimized\")\n",
    "      logStep('15 - Export the model to an HDF5 file')\n",
    "      filename = 'Output/F1_model.h5'\n",
    "      nn_model.save(filename)\n",
    "      printSeparator()\n",
    "      print('Model saved to file : ',filename)\n",
    "      printSeparator()\n",
    "      print('End of processing')\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf85e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(filename)\n",
    "predictions = model.predict(X_test_scaled[:5])\n",
    "\n",
    "target_names = results_df['Driver']\n",
    "print(target_names.head(5))\n",
    "# Print the predictions\n",
    "pprint(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8cf0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
